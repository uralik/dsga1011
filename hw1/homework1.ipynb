{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use textacy which is a lib on top of spacy to do some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: more_itertools in /misc/vlgscratch4/ChoGroup/kulikov/venv/parlai/lib/python3.6/site-packages (4.3.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.0.0 in /misc/vlgscratch4/ChoGroup/kulikov/venv/parlai/lib/python3.6/site-packages/six-1.11.0-py3.6.egg (from more_itertools) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "# run if you dont have it installed\n",
    "!pip install more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next cell is loading the training data disregarding NGRAM size and max vocabulary/max seq len size, it needs to be run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707fc51b018c42728b5c519a9271ba45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe31d25cf4e24a7dac94b22b779d27b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive training samples : 12500 \n",
      "Negaitve training samples : 12500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fcf5eb133f4b56a54e943724dbd90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f43792c06fe438ca7e8ebff4f3a211f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017d8192a6d34bdf8be7b325e257cf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee4d869c459462a9c85f37cef28ec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word vocabulary size: 92929 words\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy\n",
    "import itertools\n",
    "from operator import itemgetter \n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import more_itertools as mit  # not built-in package\n",
    "_tqdm = tqdm_notebook  # prolly you need jupyter widget for this, change for tqdm for simple tqdm\n",
    "\n",
    "# get the training data\n",
    "TRAIN_FILES_POS = glob('/home/kulikov/vlgwork/aclImdb/train/pos/*')\n",
    "pos_train_texts = []\n",
    "TRAIN_FILES_NEG = glob('/home/kulikov/vlgwork/aclImdb/train/neg/*')\n",
    "neg_train_texts = []\n",
    "\n",
    "# get training text in RAM\n",
    "for fname in _tqdm(TRAIN_FILES_NEG):\n",
    "    with open(fname, 'r') as f:\n",
    "        neg_train_texts.append(f.read())\n",
    "for fname in _tqdm(TRAIN_FILES_POS):\n",
    "    with open(fname, 'r') as f:\n",
    "        pos_train_texts.append(f.read())\n",
    "        \n",
    "print(\"Positive training samples : {} \\nNegaitve training samples : {}\".format(len(pos_train_texts), len(neg_train_texts)))\n",
    "\n",
    "TRAIN_SIZE=10000  # change this if you want\n",
    "                       \n",
    "# Split training data on train valid parts now\n",
    "pos_valid_texts = pos_train_texts[TRAIN_SIZE:]\n",
    "pos_train_texts = pos_train_texts[:TRAIN_SIZE]\n",
    "neg_valid_texts = neg_train_texts[TRAIN_SIZE:]\n",
    "neg_train_texts = neg_train_texts[:TRAIN_SIZE]\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "TAG_RE = re.compile(r'<[^>]+>') # get rid off HTML tags from the data\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)] #and (token.is_stop is False)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in _tqdm(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "    return token_dataset, all_tokens\n",
    "                       \n",
    "#clean from html tags\n",
    "pos_valid_texts = [remove_tags(t) for t in pos_valid_texts]\n",
    "neg_valid_texts = [remove_tags(t) for t in neg_valid_texts]\n",
    "pos_train_texts = [remove_tags(t) for t in pos_train_texts]\n",
    "neg_train_texts = [remove_tags(t) for t in neg_train_texts]\n",
    "\n",
    "pos_valid_texts_toked, n1 =  tokenize_dataset(pos_valid_texts)\n",
    "neg_valid_texts_toked, n2 =  tokenize_dataset(neg_valid_texts)\n",
    "pos_train_texts_toked, n3 =  tokenize_dataset(pos_train_texts)\n",
    "neg_train_texts_toked, n4 =  tokenize_dataset(neg_train_texts)\n",
    "                       \n",
    "voc = list(set(n1 + n2 + n3 + n4))\n",
    "print('Word vocabulary size: {} words'.format(len(voc)))\n",
    "                       \n",
    "def find_ngrams(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        result_list.append(list(zip(*[l[i:] for i in range(n)])))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW we split on ngrams, so change this accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM = 2  # change this to make different N grams for each token\n",
    "# now make train and valid dicts\n",
    "\n",
    "train_data = {'pos': find_ngrams(pos_train_texts_toked, NGRAM),\n",
    "              'neg': find_ngrams(neg_train_texts_toked, NGRAM)}\n",
    "valid_data = {'pos': find_ngrams(pos_valid_texts_toked, NGRAM),\n",
    "             'neg': find_ngrams(neg_valid_texts_toked, NGRAM)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocabulary from all training data (including validation part), NOW we specify max voc size and put UNK for all rare tokens\n",
    "We also keep information about frquencies in order to be able to reduce voc size later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 61897 ; token ('own', 'that')\n",
      "Token ('own', 'that'); token id 61897\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 100000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "all_train_tokens = list(mit.flatten(train_data['pos'] + train_data['neg'] + valid_data['pos'] + valid_data['neg']))\n",
    "counted_tokens = Counter(all_train_tokens)\n",
    "\n",
    "vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n",
    "id2token = list(vocab)\n",
    "token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "id2token = ['<pad>', '<unk>'] + id2token\n",
    "token2id['<pad>'] = PAD_IDX \n",
    "token2id['<unk>'] = UNK_IDX\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "def _text2id(doc):\n",
    "    return [token2id[t] if t in token2id else UNK_IDX for t in doc]\n",
    "\n",
    "def _id2text(vec):\n",
    "    return [id2token[i] for i in vec]\n",
    "    \n",
    "train_data_id = {}\n",
    "valid_data_id = {}\n",
    "\n",
    "train_data_id['pos'] = []\n",
    "for d in train_data['pos']:\n",
    "    train_data_id['pos'].append(_text2id(d))\n",
    "    \n",
    "train_data_id['neg'] = []\n",
    "for d in train_data['neg']:\n",
    "    train_data_id['neg'].append(_text2id(d))\n",
    "    \n",
    "valid_data_id['pos'] = []\n",
    "for d in valid_data['pos']:\n",
    "    valid_data_id['pos'].append(_text2id(d))\n",
    "    \n",
    "valid_data_id['neg'] = []\n",
    "for d in valid_data['neg']:\n",
    "    valid_data_id['neg'].append(_text2id(d))\n",
    "    \n",
    "train_data_id_merged = []\n",
    "valid_data_id_merged = []\n",
    "\n",
    "for d in train_data_id['pos']:\n",
    "    train_data_id_merged.append((d, 0))\n",
    "for d in train_data_id['neg']:\n",
    "    train_data_id_merged.append((d, 1))\n",
    "    \n",
    "for d in valid_data_id['pos']:\n",
    "    valid_data_id_merged.append((d, 0))\n",
    "for d in valid_data_id['neg']:\n",
    "    valid_data_id_merged.append((d, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making pytorch Dataset out of our set of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data_list, max_inp_length=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_len = max_inp_length\n",
    "        self.data_tensors = []\n",
    "        for (i, t) in tqdm_notebook(self.data):\n",
    "            \n",
    "            self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), torch.LongTensor([t]).to(device)))\n",
    "              \n",
    "    def __getitem__(self, key):\n",
    "        (inp, tgt) = self.data_tensors[key]\n",
    "        \n",
    "        return inp, tgt, len(inp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def pad(tensor, length, dim=0, pad=0):\n",
    "    \"\"\"Pad tensor to a specific length.\n",
    "    :param tensor: vector to pad\n",
    "    :param length: new length\n",
    "    :param dim: (default 0) dimension to pad\n",
    "    :returns: padded tensor if the tensor is shorter than length\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) < length:\n",
    "        return torch.cat(\n",
    "            [tensor, tensor.new(*tensor.size()[:dim],\n",
    "                                length - tensor.size(dim),\n",
    "                                *tensor.size()[dim + 1:]).fill_(pad)],\n",
    "            dim=dim)\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def batchify(batch):\n",
    "    maxlen = max(batch, key = itemgetter(2))[-1]\n",
    "    batch_list = []\n",
    "    target_list = []\n",
    "    for b in batch:\n",
    "        batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))\n",
    "        target_list.append(b[1])\n",
    "    input_batch = torch.stack(batch_list, 0)\n",
    "    target_batch = torch.stack(target_list, 0)\n",
    "    \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14de8ea9fded459a82820356682b2cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9710cf639541708759e0db7e141b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImdbDataset(train_data_id_merged, max_inp_length=None, device='cuda')\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, collate_fn=batchify, shuffle=True)\n",
    "\n",
    "valid_dataset = ImdbDataset(valid_data_id_merged, max_inp_length=None, device='cuda')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating simple model for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfNGrams(\n",
       "  (embedding): EmbeddingBag(100002, 30, mode=mean)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=30, out_features=2048, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.0)\n",
       "    (3): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BagOfNGrams(nn.Module):\n",
    "    def init_layers(self):\n",
    "        for l in self.layers:\n",
    "            if getattr(l, 'weight', None) is not None:\n",
    "                torch.nn.init.xavier_uniform_(l.weight)\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim=300, hidden_size=512, reduce='sum', nlayers=2, act='ReLU', nclasses=2, dropout=0.1, batch_norm=False):\n",
    "        super(BagOfNGrams, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.reduce = reduce\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.act = getattr(nn, act)\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_dim, mode=reduce)\n",
    "        if batch_norm is True:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.emb_dim)\n",
    "        #self.layers = nn.ModuleList([nn.Linear(self.emb_dim, 1)])\n",
    "        self.layers = nn.ModuleList([nn.Linear(self.emb_dim, self.hidden_size)])\n",
    "        self.layers.append(self.act())\n",
    "        self.layers.append(nn.Dropout(p=dropout))\n",
    "        for i in range(self.nlayers-2):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(self.act())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.layers.append(nn.Linear(self.hidden_size, 1))\n",
    "        self.init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        postemb = self.embedding(x)\n",
    "        if hasattr(self, 'batch_norm'):\n",
    "            x = self.batch_norm(postemb)\n",
    "        else:\n",
    "            x = postemb\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BagOfNGrams(len(id2token), emb_dim=30, hidden_size=2048, act='Tanh', nlayers=1, reduce='mean', dropout=0.0, batch_norm=False)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/20], Step: [40/40], Train loss: 6.676809787750244, Validation Acc: 51.16\n",
      "Epoch: [11/20], Step: [40/40], Train loss: 0.7558750510215759, Validation Acc: 77.96\n",
      "Epoch: [16/20], Step: [40/40], Train loss: 3.158962135785259e-05, Validation Acc: 87.52\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='elementwise_mean')\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, nesterov=True)\n",
    "#optimizer = torch.optim.Adagrad(params=model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), amsgrad=True, lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels in loader:\n",
    "        outputs = torch.sigmoid(model(data))\n",
    "        predicted = (outputs > 0.5).long()\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader): \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        loss = criterion(outputs.view(-1), labels.float().view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #print('Train loss: {}'.format(loss.item()))\n",
    "        # validate every 100 iterations\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        val_acc = test_model(loader=valid_loader, model=model)\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Train loss: {}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss.item(), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37df2650b9ff4c35a59f8e5933b197db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625b0d56f2d34878a6b02f4d5b57ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive training samples : 12500 \n",
      "Negaitve training samples : 12500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41232ca4bc594cc887849c115eb247f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4824717ae31e4933b43d01a6271b3718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        result_list.append([list(zip(*[l[i:] for i in range(n)])), ' '.join(l)])\n",
    "    return result_list\n",
    "\n",
    "# get the training data\n",
    "TEST_FILES_POS = glob('/home/kulikov/vlgwork/aclImdb/test/pos/*')\n",
    "pos_test_texts = []\n",
    "TEST_FILES_NEG = glob('/home/kulikov/vlgwork/aclImdb/test/neg/*')\n",
    "neg_test_texts = []\n",
    "\n",
    "# get training text in RAM\n",
    "for fname in _tqdm(TEST_FILES_NEG):\n",
    "    with open(fname, 'r') as f:\n",
    "        neg_test_texts.append(f.read())\n",
    "for fname in _tqdm(TEST_FILES_POS):\n",
    "    with open(fname, 'r') as f:\n",
    "        pos_test_texts.append(f.read())\n",
    "        \n",
    "print(\"Positive training samples : {} \\nNegaitve training samples : {}\".format(len(pos_test_texts), len(neg_test_texts)))\n",
    "\n",
    "#clean from html tags\n",
    "pos_test_texts = [remove_tags(t) for t in pos_test_texts]\n",
    "neg_test_texts = [remove_tags(t) for t in neg_test_texts]\n",
    "\n",
    "pos_test_texts_toked, n1 =  tokenize_dataset(pos_test_texts)\n",
    "neg_test_texts_toked, n2 =  tokenize_dataset(neg_test_texts)\n",
    "\n",
    "test_data = {'pos': find_ngrams(pos_test_texts_toked, NGRAM),\n",
    "              'neg': find_ngrams(neg_test_texts_toked, NGRAM)}\n",
    "\n",
    "test_data_id = {}\n",
    "test_data_id['pos'] = []\n",
    "for d in test_data['pos']:\n",
    "    test_data_id['pos'].append([_text2id(d[0]), d[1]])\n",
    "    \n",
    "test_data_id['neg'] = []\n",
    "for d in test_data['neg']:\n",
    "    test_data_id['neg'].append([_text2id(d[0]), d[1]])\n",
    "    \n",
    "test_data_id_merged = []\n",
    "\n",
    "for d in test_data_id['pos']:\n",
    "    test_data_id_merged.append((d[0], 0, d[1]))\n",
    "for d in test_data_id['neg']:\n",
    "    test_data_id_merged.append((d[0], 1, d[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDatasetWithText(Dataset):\n",
    "    def __init__(self, data_list, max_inp_length=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_len = max_inp_length\n",
    "        self.data_tensors = []\n",
    "        self.text = []\n",
    "        for (i, t, text) in tqdm_notebook(self.data):\n",
    "            self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), torch.LongTensor([t]).to(device)))\n",
    "            self.text.append(text)\n",
    "              \n",
    "    def __getitem__(self, key):\n",
    "        (inp, tgt) = self.data_tensors[key]\n",
    "        text = self.text[key]\n",
    "        \n",
    "        return inp, tgt, len(inp), text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def pad(tensor, length, dim=0, pad=0):\n",
    "    \"\"\"Pad tensor to a specific length.\n",
    "    :param tensor: vector to pad\n",
    "    :param length: new length\n",
    "    :param dim: (default 0) dimension to pad\n",
    "    :returns: padded tensor if the tensor is shorter than length\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) < length:\n",
    "        return torch.cat(\n",
    "            [tensor, tensor.new(*tensor.size()[:dim],\n",
    "                                length - tensor.size(dim),\n",
    "                                *tensor.size()[dim + 1:]).fill_(pad)],\n",
    "            dim=dim)\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def batchify(batch):\n",
    "    maxlen = max(batch, key = itemgetter(2))[-2]\n",
    "    batch_list = []\n",
    "    target_list = []\n",
    "    text_list = []\n",
    "    for b in batch:\n",
    "        batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))\n",
    "        target_list.append(b[1])\n",
    "        text_list.append(b[3])\n",
    "    input_batch = torch.stack(batch_list, 0)\n",
    "    target_batch = torch.stack(target_list, 0)\n",
    "    \n",
    "    return input_batch, target_batch, text_list\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels, text in loader:\n",
    "        outputs = torch.sigmoid(model(data))\n",
    "        predicted = (outputs > 0.5).long()\n",
    "        #import ipdb; ipdb.set_trace() # use this to get examples below\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14490a9b1ded4376aa236614aeffba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImdbDatasetWithText(test_data_id_merged, max_inp_length=None, device='cuda')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(loader=test_loader, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correctly predicted samples\n",
    "\n",
    "### Positive\n",
    "`it 's hard to know what to make of this weird little aussie crime flick on the one hand it 's an enjoyable little film with a great sense of humour but on the other it just lacks a certain something that ensures the film never reaches above it 's boundary that keeps it trapped within the merely interesting territory that being said two hands is a well plotted film that excellently juggles several stories at the same time which allows several small climaxes throughout the movie and that in turn helps to stop the film becoming boring the absurdity of the goings on the thick australian accents and the bizarre set of characters all help to ensure that the film entertains also the plot follows the story of a young doorman who thinks he 'll go on to bigger things after accepting a job from the local kingpin he does n't the job only lands him in trouble when he fancies a swim and stupidly leaves ten grand on the beach which is promptly stolen by a couple of kids who have the time of their lives on a shopping spree however all is not rosy for our hero who must find the money or face the consequences ... the film is made up of a cast of unknowns at least it was back in 1999 as nowadays heath ledger is something of a name he does n't impress too much here however as his performance is mostly of the one note variety and he does n't make for a very compelling lead he fits the movie in that he 's australian and looks naive but beyond that he 's not the best lead i 've ever seen in a movie if you ask me bryan brown gave the best performance here he might not have a great deal of screen time but he steals every scene he 's in and it 's him that provides the movie with a lot of its humour he 's got nothing to do with the best sequence however which takes place in the form of probably the most hilarious bank robbery ever caught on film on the whole i can recommend this film to people that enjoy quirky crime films as the weirdness is plentiful and the way that events take a turn for the bizarre is enjoyable but if you 're not a fan of this sort of film i ca n't really say that two hands will float your boat it 's not a must see but if it 's your thing and you get a chance to see it ... you probably wo nt completely regret it`\n",
    "\n",
    "### Positive\n",
    "`sally and saint anne is a very funny movie the first time my mom told me about it i was 7 and saint anne had just been the saint i had for my communion saint my mom knew this so she told me to watch this with her i did and have seen it many times since because it is really funny aunt bea from the andy griffith show was in it and sally 's grandfather was the guy who played santa claus in miracle on 34th street so there were lots of actors we seen on tv shows too there is a bad guy who keeps trying to steal the house away and sally keeps trying things with st. anne to help raise money so they can keep the house that includes a boxing match with hugh o'brian who plays her older brother this is a good and funny movie that i still love`\n",
    "\n",
    "### Negative\n",
    "`unless there 's some huge ironic conspiracy going on my jaw dropped when i read the positive reviews of this film i can not believe that this film was even released it 's so bad i admit it is not my kind of movie but i tried to watch it objectively anyway you know so bad it 's funny and was still offended at its sheer awfulness the acting is atrocious they ca n't have watched the rushes and i 'm guessing there was one take per scene it really is that terrible it is the worst film i have seen in many a year in fact i would n't even call it a film it 's a tragedy the gay black friend whom no one actually calls gay it 's just implied because he 's so crazy homophobic this is not good in fact this is downright vomit inducing the jokes die on their pathetic arses the music is so bad it defies belief the person who compiled the soundtrack essentially chose the most ear mutilatingly bad songs they had ever heard and put them in this waste of film stock oh my good christ i ca n't believe the 80 's produced utter garbage like this i grew up through them and i can not find one thing worth of note here it must have been a dark time to be a cinema goer if you even contemplate watching this film go see a psychiatrist he will then accordingly slap you you sick sick person`\n",
    "\n",
    "## incorrectly predicted samples\n",
    "\n",
    "### Positive (predicted as negative)\n",
    "`ok when i rented this several years ago i had the worst expectations yes the acting is n't great and the picture itself looks dated but as i sat there a strange thing happened i started to like it the action is great and there are few scenes that make you jump brion james maybe one of the greatest b grade actors next to bruce campbell is great as always the story is n't bad either now i would n't rush out and buy it but you wo n't waste your time at least watching this good b grade post apocalyptic western`\n",
    "\n",
    "### Negative (predicted as positive)\n",
    "`i first saw this movie at a saturday matinee when i was very young i thought it was cool and often thought about it well i finally resaw it on dvd it was still very entertaining but in a different way it has to rank as one of the goofiest campiest 1950 's sci fi movies it seemed filled with stock military footage the dialogue is stilted and effects are crude there is one line of dialogue that had me in stitches the line jeff morrow says while on the beach with the babe rent it if you need a movie to watch with a bunch of drunken friends it is a classic`\n",
    "\n",
    "### Positive (predicted as negative)\n",
    "`look ... i\\'ve come to expect this level of acting from william macy ... the guy just keeps putting in terrific performances ... but meat loaf just when did his loafness decide to leave jim steinman behind and throw his decidedly lower weight around in the wonderful world of stanislavsky well ... what can i say i \\'m duly impressed to paraphrase an old adage it ai n\\'t the meat it \\'s the emotion\" ... and the loaf is quietly buffing up his acting chops of late .. laura dern carries off the 40 \\'s look perfectly here ... great job by the costume and hair departments ... david paymer is typecast but right on the money solid camera work throughout the flick the plot line is reminiscent of gentleman \\'s agreement post wwii anti semitism well worth your time ... particularly for the growing legions of bill macy acolytes`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
